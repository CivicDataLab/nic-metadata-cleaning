{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c1087d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple\n",
      "is\n",
      "n't\n",
      "looking\n",
      "at\n",
      "buying\n",
      "U.K.\n",
      "startup\n",
      "for\n",
      "$\n",
      "1\n",
      "billion\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import spacy\n",
    "nlp=spacy.load(\"en_core_web_sm\")\n",
    "doc=nlp(\"Apple isn't looking at buying U.K. startup for $1 billion\")\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b063d1",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e541c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Apple PROPN NNP nsubj Xxxxx True False\n",
      "is be AUX VBZ aux xx True True\n",
      "n't not PART RB neg x'x False True\n",
      "looking look VERB VBG ROOT xxxx True False\n",
      "at at ADP IN prep xx True True\n",
      "buying buy VERB VBG pcomp xxxx True False\n",
      "U.K. U.K. PROPN NNP nsubj X.X. False False\n",
      "startup startup VERB VBD ccomp xxxx True False\n",
      "for for ADP IN prep xxx True True\n",
      "$ $ SYM $ quantmod $ False False\n",
      "1 1 NUM CD compound d False False\n",
      "billion billion NUM CD pobj xxxx True False\n"
     ]
    }
   ],
   "source": [
    "#pos tags and dependencies\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c5eb778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple 0 5 ORG\n",
      "U.K. 30 34 GPE\n",
      "$1 billion 47 57 MONEY\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53ad68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gimme', 'that']\n",
      "['gim', 'me', 'that']\n"
     ]
    }
   ],
   "source": [
    "from spacy.symbols import ORTH\n",
    "doc=nlp(\"gimme that\")\n",
    "print([w.text for w in doc])\n",
    "\n",
    "# add special case rule\n",
    "special_case=[{ORTH:\"gim\"}, {ORTH:\"me\"}]\n",
    "nlp.tokenizer.add_special_case(\"gimme\", special_case)\n",
    "\n",
    "# check new tokenization after adding the special case rule\n",
    "print([w.text for w in nlp(\"gimme that\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd48ec20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('SPECIAL-1', 'Let'), ('SPECIAL-2', \"'s\"), ('TOKEN', 'Go'), ('SUFFIX', '!')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.tokenizer.explain(\"Let's Go!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "732d11e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" \\t PREFIX\n",
      "Let \\t SPECIAL-1\n",
      "'s \\t SPECIAL-2\n",
      "go \\t TOKEN\n",
      "! \\t SUFFIX\n",
      "\" \\t SUFFIX\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "text = '''\"Let's go!\"'''\n",
    "doc = nlp(text)\n",
    "tok_exp = nlp.tokenizer.explain(text)\n",
    "assert [t.text for t in doc if not t.is_space] == [t[1] for t in tok_exp]\n",
    "for t in tok_exp:\n",
    "    print(t[1], \"\\\\t\", t[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57027fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "special_cases = {\":)\": [{\"ORTH\": \":)\"}]}\n",
    "prefix_re = re.compile(r'''^[\\\\[\\\\(\"']''')\n",
    "# suffix_re = re.compile(r'''[\\\\]\\\\)\"']$''')\n",
    "infix_re = re.compile(r'''[-~]''')\n",
    "simple_url_re = re.compile(r'''^https?://''')\n",
    "\n",
    "def custom_tokenizer(nlp):\n",
    "    return Tokenizer(nlp.vocab, rules=special_cases,\n",
    "                                prefix_search=prefix_re.search,\n",
    "                                # suffix_search=suffix_re.search,\n",
    "                                infix_finditer=infix_re.finditer,\n",
    "                                url_match=simple_url_re.match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc03abcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', '-', 'world.', ':)']\n"
     ]
    }
   ],
   "source": [
    "nlp.tokenizer=custom_tokenizer(nlp)\n",
    "doc=nlp(\"hello-world. :)\")\n",
    "print([t.text for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ed35cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "suffixes=nlp.Defaults.suffixes+[r'''-+$''',]\n",
    "suffix_regex=spacy.util.compile_suffix_regex(suffixes)\n",
    "nlp.tokenizer.suffix_search=suffix_regex.search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fd22f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "suffixes=list(nlp.Defaults.suffixes)\n",
    "suffixes.remove(\"\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d00ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mother', '-', 'in', '-', 'lay']\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.char_classes import ALPHA, ALPHA_LOWER, ALPHA_UPPER\n",
    "from spacy.lang.char_classes import CONCAT_QUOTES, LIST_ELLIPSES, LIST_ICONS\n",
    "nlp=spacy.load(\"en_core_web_sm\")\n",
    "doc=nlp(\"mother-in-lay\")\n",
    "print([t.text for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896915af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mpdify_tokenizer_infix\n",
    "# Modify tokenizer infix patterns\n",
    "infixes = (\n",
    "    LIST_ELLIPSES\n",
    "    + LIST_ICONS\n",
    "    + [\n",
    "        r\"(?<=[0-9])[+\\\\-\\\\*^](?=[0-9-])\",\n",
    "        r\"(?<=[{al}{q}])\\\\.(?=[{au}{q}])\".format(\n",
    "            al=ALPHA_LOWER, au=ALPHA_UPPER, q=CONCAT_QUOTES\n",
    "        ),\n",
    "        r\"(?<=[{a}]),(?=[{a}])\".format(a=ALPHA),\n",
    "        # âœ… Commented out regex that splits on hyphens between letters:\n",
    "        # r\"(?<=[{a}])(?:{h})(?=[{a}])\".format(a=ALPHA, h=HYPHENS),\n",
    "        r\"(?<=[{a}0-9])[:<>=/](?=[{a}])\".format(a=ALPHA),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "65a5831d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's go!\n",
      "<spacy.strings.StringStore object at 0x76993e9e5380>\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Doc\n",
    "words=[\"Let\",\"'s\", \"go\", \"!\"]\n",
    "spaces=[False, True, False, False]\n",
    "doc=Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc)\n",
    "print(nlp.vocab.strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4842307e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"What's\", 'happened', 'to', 'me?', 'he', 'thought.', 'It', \"wasn't\", 'a', 'dream.']\n"
     ]
    }
   ],
   "source": [
    "# basic whitespace tokenizer\n",
    "class WhitespaceTokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab=vocab\n",
    "\n",
    "    def __call__(self, text):\n",
    "        words=text.split(\" \")\n",
    "        spaces=[True]*(len(words))\n",
    "\n",
    "        # avoid zero length tokens\n",
    "        for i, word in enumerate(words):\n",
    "            if word==\"\":\n",
    "                words[i]=\" \"\n",
    "                spaces[i]=False\n",
    "        # Remove the final trailing space\n",
    "        if words[-1]==\" \":\n",
    "            words=words[0:-1]\n",
    "            spaces=spaces[0:-1]\n",
    "        else:\n",
    "            spaces[-1]=False\n",
    "\n",
    "        return Doc(self.vocab, words=words, spaces=spaces)\n",
    "    \n",
    "nlp=spacy.blank(\"en\")\n",
    "nlp.tokenizer=WhitespaceTokenizer(nlp.vocab)\n",
    "doc=nlp(\"What's happened to me? he thought. It wasn't a dream.\")\n",
    "print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cd7c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "class BertTokenizer:\n",
    "    def __init__(self, vocab, vocab_file, lowercase=True):\n",
    "\n",
    "        self.vocab=vocab\n",
    "        self._tokenizer=BertWordPieceTokenizer(vocab_file, lowercase=lowercase)\n",
    "\n",
    "    def __call__(self, text):\n",
    "        tokens=self._tokenizer.encode(text)\n",
    "        words=[]\n",
    "        spaces=[]\n",
    "        for i, (text, (start,end)) in enumerate()\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nic-metadata-cleaning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
