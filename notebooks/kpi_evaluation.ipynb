{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9234d5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "excel_file = '/home/aakash/NIC/nic-metadata-cleaning/sample_catalog_metadata/NIC_metadata_curation_master.xlsx'\n",
    "catalog_df = pd.read_csv('/home/aakash/NIC/nic-metadata-cleaning/sample_catalog_metadata/nic_sample_catalog.csv')\n",
    "resource_df = pd.read_csv('/home/aakash/NIC/nic-metadata-cleaning/sample_datasets_metadata/nic_sample_dataset.csv')\n",
    "\n",
    "print(\"Catalog metadata columns:\", catalog_df.columns.tolist())\n",
    "print(\"Resource metadata columns:\", resource_df.columns.tolist())\n",
    "\n",
    "print(\"\\nFirst catalog entry:\\n\", catalog_df.iloc[0].to_dict())\n",
    "print(\"\\nFirst resource entry:\\n\", resource_df.iloc[0].to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080e56df",
   "metadata": {},
   "source": [
    "KPI Evaluation Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a7c307",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190761d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_catalog_row(row):\n",
    "    results={}\n",
    "    required_fields = ['title', 'body:value', 'published_date','field_ministry_department:name', 'node_alias']\n",
    "    all_fields_present = all(pd.notna(row[f]) and str(row[f]).strip() != \"\" for f in required_fields)\n",
    "    results['FieldCoverage'] = 1 if all_fields_present else 0\n",
    "    has_distribution = row['title'] in set(resource_df['catalog_title'])\n",
    "    results['DCATCompliance'] = 1 if (all_fields_present and has_distribution) else 0\n",
    "    controlled = True\n",
    "    if pd.isna(row.get('field_asset_jurisdiction:name')) or str(row['field_asset_jurisdiction:name']).strip() == \"\":\n",
    "        controlled = False\n",
    "    if pd.isna(row.get('field_ds_govt_type')) or str(row['field_ds_govt_type']).strip() == \"\":\n",
    "        controlled = False\n",
    "    if pd.isna(row.get('field_ministry_department:name')) or str(row['field_ministry_department:name']).strip() == \"\":\n",
    "        controlled = False\n",
    "    if pd.isna(row.get('field_sector:name')) or str(row['field_sector:name']).strip() == \"\":\n",
    "        controlled = False\n",
    "    results['ControlledVocab'] = 1 if controlled else 0\n",
    "    # 4. Jurisdiction-Level \n",
    "    jur = row.get('field_asset_jurisdiction:name')\n",
    "    results['JurisdictionComplete'] = 1 if (pd.notna(jur) and str(jur).strip() != \"\") else 0\n",
    "    # 5. Legislation Field Coverage\n",
    "\n",
    "    results['LegislationCoverage'] = 0 # e.g., would be 1 if an 'applicableLegislation' field is not empty\n",
    "    # # Check if there's at least one distribution with a valid link and format\n",
    "    dataset_title = row['title']\n",
    "    dist_entries = resource_df[resource_df['catalog_title'] == dataset_title]\n",
    "    if len(dist_entries) > 0:\n",
    "        valid_dist = False\n",
    "        for _, res in dist_entries.iterrows():\n",
    "            has_link = (pd.notna(res['datafile']) or pd.notna(res['datafile_url']))\n",
    "            has_format = pd.notna(res['file_format']) and str(res['file_format']).strip() != \"\"\n",
    "            if has_link and has_format:\n",
    "                valid_dist = True\n",
    "                break\n",
    "            results['DistributionIntegrity'] = 1 if valid_dist else 0\n",
    "    else:\n",
    "        results['DistributionIntegrity'] = 0\n",
    "\n",
    "    # 7. Multilingual Metadata Coverage (dataset-level)\n",
    "    title_text = str(row['title']) if pd.notna(row['title']) else \"\"\n",
    "    desc_text = str(row['body:value']) if pd.notna(row['body:value']) else \"\"\n",
    "    def contains_non_ascii(text):\n",
    "        return any(ord(ch) > 127 for ch in text)\n",
    "    multilingual = contains_non_ascii(title_text) or contains_non_ascii(desc_text)\n",
    "    results['Multilingual'] = 1 if multilingual else 0\n",
    "\n",
    "\n",
    "    # Metadata-to-Data Link Validity \n",
    "    if len(dist_entries) > 0:\n",
    "        all_links_ok = True\n",
    "        for _, res in dist_entries.iterrows():\n",
    "            # Check each distribution's link format\n",
    "            url = None\n",
    "            if pd.notna(res['datafile_url']):\n",
    "                url = str(res['datafile_url'])\n",
    "            elif pd.notna(res['datafile']):\n",
    "                url = str(res['datafile'])\n",
    "            if not (url and (url.startswith('http://') or\n",
    "                             url.startswith('https://'))):\n",
    "                all_links_ok = False\n",
    "        results['LinkValidity'] = 1 if all_links_ok else 0\n",
    "    else:\n",
    "        results['LinkValidity'] = 0\n",
    "\n",
    "    # 10. Metadata Coverage for Dublin Core (dataset-level)\n",
    "    dc_fields = ['title', 'body:value', 'published_date',\n",
    "                 'field_ministry_department:name', 'keywords']\n",
    "    dc_fields_present = all(pd.notna(row[f]) and str(row[f]).strip() != \"\" for f in dc_fields)\n",
    "    results['DublinCoreCoverage'] = 1 if dc_fields_present else 0\n",
    "    # 11. Catalog Sector Field Accuracy\n",
    "    sector_val = str(row.get('field_sector:name', \"\")).strip()\n",
    "    # Mark accurate if sector is not empty and not a generic \"All\"\n",
    "    if sector_val == \"\" or sector_val.lower() == \"all\":\n",
    "        results['SectorAccuracy'] = 0\n",
    "    else:\n",
    "        results['SectorAccuracy'] = 1\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2f2470",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_resource_row(row):\n",
    "    results = {}\n",
    "    # 1. Metadata Field Coverage (resource-level)\n",
    "    # Required: title, format, and a data link (datafile or datafile_url)\n",
    "    has_title = pd.notna(row['title']) and str(row['title']).strip() != \"\"\n",
    "    has_format = pd.notna(row['file_format']) and str(row['file_format']).strip() != \"\"\n",
    "    has_data_link = pd.notna(row['datafile']) or pd.notna(row['datafile_url'])\n",
    "    results['FieldCoverage'] = 1 if (has_title and has_format and has_data_link) else 0\n",
    "    # 2. DCAT Compliance (resource-level)\n",
    "    #DCAT compliance is exact matched to Dublin and DCAT seperate\n",
    "    results['DCATCompliance'] = 1 if (has_title and has_format and has_data_link) else 0\n",
    "    # 3. Controlled Vocabulary Usage (resource-level)\n",
    "    controlled = True\n",
    "\n",
    "    if not (pd.notna(row['file_format']) and \"/\" in str(row['file_format'])):\n",
    "        controlled = False\n",
    "\n",
    "    if pd.isna(row.get('resource_category')) or str(row['resource_category']).strip() == \"\":\n",
    "        controlled = False\n",
    "    results['ControlledVocab'] = 1 if controlled else 0\n",
    "    # 4. Jurisdiction-Level Completeness (resource-level)\n",
    "    jur_ok = True\n",
    "    if pd.isna(row.get('govt_type')) or str(row['govt_type']).strip() == \"\":\n",
    "        jur_ok = False\n",
    "    elif str(row['govt_type']).strip().lower() == 'state':\n",
    "\n",
    "        if pd.isna(row.get('state_department')) or str(row['state_department']).strip() == \"\":\n",
    "            jur_ok = False\n",
    "    results['JurisdictionComplete'] = 1 if jur_ok else 0\n",
    "    # 5. Legislation Field Coverage (resource-level)\n",
    "    results['LegislationCoverage'] = 0\n",
    "    # 6. Dataset Distribution Integrity (resource-level)\n",
    "    # For an individual resource, this means the resource itself is valid (has\n",
    "    # link and format)\n",
    "    results['DistributionIntegrity'] = 1 if (has_format and has_data_link) else 0\n",
    "\n",
    "    # 7. Multilingual Metadata Coverage (resource-level)\n",
    "    title_text = str(row['title']) if pd.notna(row['title']) else \"\"\n",
    "    note_text = str(row['note']) if pd.notna(row['note']) else \"\" # 'note'\n",
    "    # could be description of resource\n",
    "    multilingual = any(ord(ch) > 127 for ch in title_text) or any(ord(ch) > 127 for ch in note_text)\n",
    "    results['Multilingual'] = 1 if multilingual else 0\n",
    "    # 8. Metadata Harvestability (resource-level)\n",
    "\n",
    "        # 8. Harvestability\n",
    "    api_flag = str(row.get('external_api_reference') or \"\").strip().lower()\n",
    "    fmt = str(row.get('file_format') or \"\").strip().lower()\n",
    "\n",
    "    if (api_flag and api_flag != 'nan') or ('json' in fmt):\n",
    "        results['Harvestability'] = 1\n",
    "    else:\n",
    "        results['Harvestability'] = 0\n",
    "    # 9. Metadata-to-Data Link Validity (resource-level)\n",
    "    url = None\n",
    "    if pd.notna(row['datafile_url']):\n",
    "        url = str(row['datafile_url'])\n",
    "    elif pd.notna(row['datafile']):\n",
    "        url = str(row['datafile'])\n",
    "    link_ok = url is not None and (url.startswith('http://') or url.startswith('https://'))\n",
    "    results['LinkValidity'] = 1 if link_ok else 0\n",
    "    # 10. Metadata Coverage for Dublin Core (resource-level)\n",
    "\n",
    "    results['DublinCoreCoverage'] = 1 if (has_title and has_format) else 0\n",
    "    # 11. Catalog Sector Field Accuracy (resource-level)\n",
    "    sector_val = str(row.get('sector', \"\")).strip()\n",
    "    if sector_val == \"\" or sector_val.lower() == \"all\":\n",
    "        results['SectorAccuracy'] = 0\n",
    "    else:\n",
    "        results['SectorAccuracy'] = 1\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2bf499",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the evaluation to each row of the DataFrames\n",
    "catalog_scores = catalog_df.apply(evaluate_catalog_row, axis=1,result_type='expand')\n",
    "resource_scores = resource_df.apply(evaluate_resource_row, axis=1,result_type='expand')\n",
    "# Calculate cumulative score (sum of all KPI flags) for each entry\n",
    "catalog_scores['TotalScore'] = catalog_scores.sum(axis=1)\n",
    "resource_scores['TotalScore'] = resource_scores.sum(axis=1)\n",
    "\n",
    "# Add an identifier column (like title) for clarity in the results\n",
    "catalog_scores.insert(0, 'CatalogTitle', catalog_df['title'])\n",
    "resource_scores.insert(0, 'ResourceTitle', resource_df['title'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe55ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first 5 catalog entries with their KPI scores\n",
    "print(\"KPI scores for first 5 catalog:\")\n",
    "print(catalog_scores.head(5).to_string(index=False))\n",
    "# Display first 5 resource entries with their KPI scores\n",
    "print(\"\\nKPI scores for first 5 resources:\")\n",
    "print(resource_scores.head(5).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d699b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print overall fulfillment rates for each KPI for catalogs\n",
    "total_catalogs = len(catalog_scores)\n",
    "print(\"Overall Catalog KPI Fulfillment Rates:\")\n",
    "for kpi in catalog_scores.columns:\n",
    "    if kpi in ('CatalogTitle', 'DatasetTitle', 'TotalScore', 'Category'):\n",
    "        continue\n",
    "    # Only process numeric columns\n",
    "    if pd.api.types.is_numeric_dtype(catalog_scores[kpi]):\n",
    "        fulfilled = int(catalog_scores[kpi].sum())\n",
    "        total     = len(catalog_scores)\n",
    "        print(f\"- {kpi}: {fulfilled}/{total} ({(fulfilled/total*100 if total else 0):.1f}%)\")\n",
    "# Calculate and print overall fulfillment rates for each KPI for resources\n",
    "total_resources = len(resource_scores)\n",
    "print(\"\\nOverall Resource KPI Fulfillment Rates:\")\n",
    "for kpi in resource_scores.columns[2:-1]:\n",
    "\n",
    "    fulfilled = resource_scores[kpi].sum()\n",
    "    rate = fulfilled / total_resources * 100\n",
    "    print(f\"{kpi}: {fulfilled}/{total_resources} ({rate:.1f}%)\")\n",
    "# Print average total scores\n",
    "avg_catalog_score = catalog_scores['TotalScore'].mean()\n",
    "avg_resource_score = resource_scores['TotalScore'].mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b567c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def kpi_fulfillment_summary(df, level_name, exclude_cols=(\"DatasetTitle\",\"ResourceTitle\",\"TotalScore\",\"Type\",\"HTTP_Status\")):\n",
    "    summary = []\n",
    "    kpi_cols = [c for c in df.columns if c not in exclude_cols]\n",
    "    total = len(df)\n",
    "    for kpi in kpi_cols:\n",
    "        if total == 0:\n",
    "            fulfilled, pct = 0, 0.0\n",
    "        else:\n",
    "            fulfilled = int(pd.to_numeric(df[kpi], errors=\"coerce\").fillna(0).astype(int).sum())\n",
    "            pct = (fulfilled / total) * 100\n",
    "        summary.append([level_name, kpi, fulfilled, total, round(pct,1)])\n",
    "    return summary\n",
    "\n",
    "# Generate summaries\n",
    "catalog_summary = kpi_fulfillment_summary(catalog_scores, \"Catalog\")\n",
    "resource_summary = kpi_fulfillment_summary(resource_scores, \"Resource\")\n",
    "\n",
    "# Write to CSV\n",
    "with open(\"kpi_result/kpi_fulfillment_summary.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Type\", \"KPI\", \"Fulfilled\", \"Total\", \"Percent\"])\n",
    "    writer.writerows(catalog_summary + resource_summary)\n",
    "\n",
    "print(\"Wrote kpi_fulfillment_summary.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4769c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a25d1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae217570",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5363680b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57315fc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12796f94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3fc345",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from collections import Counter\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to check HTTP status of URLs\n",
    "def check_url_status(url):\n",
    "    try:\n",
    "        response = requests.head(url, allow_redirects=True, timeout=10)\n",
    "        return response.status_code\n",
    "    except requests.RequestException as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Initialize a counter for response codes\n",
    "response_code_counter = Counter()\n",
    "\n",
    "# Iterate through all links in 'datafile' and 'datafile_url'\n",
    "for column in ['datafile', 'datafile_url']:\n",
    "    for url in resource_df[column].dropna():\n",
    "        status = check_url_status(url)\n",
    "        response_code_counter[status] += 1\n",
    "\n",
    "# Log the stats to a .log file\n",
    "log_file = \"link_stats.log\"\n",
    "now = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "with open(log_file, \"w\") as log:\n",
    "    log.write(f\"Link Statistics Log\\n\")\n",
    "    log.write(f\"Generated on: {now}\\n\\n\")\n",
    "    log.write(\"HTTP Response Code Counts:\\n\")\n",
    "    for code, count in response_code_counter.items():\n",
    "        log.write(f\"- {code}: {count}\\n\")\n",
    "\n",
    "print(f\"Link statistics logged to {log_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b53805",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import pandas as pd\n",
    "# from pathlib import Path\n",
    "\n",
    "# # Exact file name (as provided)\n",
    "# MAPPING_XLSX = Path(\"/home/aakash/NIC/nic-metadata-cleaning/sample_catalog_metadata/NIC_metadata_curation_master.xlsx\")\n",
    "\n",
    "# # Exact sheet names (as in the workbook)\n",
    "# SHEET_CAT  = \"catalog_metadata_mapping_table\"\n",
    "# SHEET_DS   = \"dataset_metadata_mapping_table\"\n",
    "# SHEET_DIST = \"distribution_metadata_mapping_t\"\n",
    "\n",
    "# # Read exactly the sheets we need\n",
    "# df_cat_map  = pd.read_excel(MAPPING_XLSX, sheet_name=SHEET_CAT)\n",
    "# df_ds_map   = pd.read_excel(MAPPING_XLSX, sheet_name=SHEET_DS)\n",
    "# df_dist_map = pd.read_excel(MAPPING_XLSX, sheet_name=SHEET_DIST)\n",
    "\n",
    "# # Keep only rows with a defined Mapping Type (as in the sheets)\n",
    "# for _df in (df_cat_map, df_ds_map, df_dist_map):\n",
    "#     _df.dropna(subset=[\"Mapping Type\"], inplace=True)\n",
    "#     _df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# def categorize_mapping(df_map: pd.DataFrame, nic_col: str):\n",
    "#     \"\"\"\n",
    "#     Bucket NIC fields into exact / partial / unmapped using:\n",
    "#     - 'Mapping Type' (exact, partial/closest, no match/no direct)\n",
    "#     - NIC field column passed explicitly (no autodetection).\n",
    "#     \"\"\"\n",
    "#     exact_fields, partial_fields, unmapped_fields = [], [], []\n",
    "\n",
    "#     for _, row in df_map.iterrows():\n",
    "#         mtype = str(row[\"Mapping Type\"]).lower()\n",
    "#         field_name = row[nic_col]\n",
    "\n",
    "#         # Normalize the field name for empty/NaN\n",
    "#         if field_name is None or (isinstance(field_name, float) and pd.isna(field_name)):\n",
    "#             field_name = \"(None)\"  # standard field exists but not implemented in NIC yet\n",
    "#         else:\n",
    "#             field_name = str(field_name).strip()\n",
    "\n",
    "#         if \"exact\" in mtype:\n",
    "#             exact_fields.append(field_name)\n",
    "#         elif \"partial\" in mtype or \"closest\" in mtype:\n",
    "#             partial_fields.append(field_name)\n",
    "#         elif \"no match\" in mtype or \"no direct\" in mtype:\n",
    "#             unmapped_fields.append(field_name)\n",
    "\n",
    "#     return exact_fields, partial_fields, unmapped_fields\n",
    "\n",
    "\n",
    "# #Catalog sheet \"Equivalent OGD Catalog Metadata\"\n",
    "# #Dataset sheet \"Equivalent OGD Dataset Metadata\"\n",
    "\n",
    "# exact_cat,  partial_cat,  unmapped_cat  = categorize_mapping(df_cat_map,  \"Equivalent OGD Catalog Metadata\")\n",
    "# exact_ds,   partial_ds,   unmapped_ds   = categorize_mapping(df_ds_map,   \"Equivalent OGD Dataset Metadata\")\n",
    "# exact_dist, partial_dist, unmapped_dist = categorize_mapping(df_dist_map, \"Equivalent OGD Dataset Metadata\")\n",
    "\n",
    "# # (Optional) Append a short summary to evaluation.log\n",
    "# from pathlib import Path\n",
    "# log_path = Path(\"evaluation.log\")\n",
    "# with log_path.open(\"a\", encoding=\"utf-8\") as log:\n",
    "#     log.write(\"## Metadata Field Mapping to Standards (DCAT v3 and Dublin Core)\\n\\n\")\n",
    "\n",
    "#     log.write(\"### Catalog Metadata Fields Mapping\\n\")\n",
    "#     log.write(f\"- Exact Matches ({len(exact_cat)}): \" + (\", \".join(exact_cat) if exact_cat else \"None\") + \"\\n\")\n",
    "#     log.write(f\"- Partial/Close Matches ({len(partial_cat)}): \" + (\", \".join(partial_cat) if partial_cat else \"None\") + \"\\n\")\n",
    "#     log.write(f\"- Problematic/Unmapped ({len(unmapped_cat)}): \" + (\", \".join(unmapped_cat) if unmapped_cat else \"None\") + \"\\n\\n\")\n",
    "\n",
    "#     log.write(\"### Dataset Metadata Fields Mapping\\n\")\n",
    "#     log.write(f\"- Exact Matches ({len(exact_ds)}): \" + (\", \".join(exact_ds) if exact_ds else \"None\") + \"\\n\")\n",
    "#     log.write(f\"- Partial/Close Matches ({len(partial_ds)}): \" + (\", \".join(partial_ds) if partial_ds else \"None\") + \"\\n\")\n",
    "#     log.write(f\"- Problematic/Unmapped ({len(unmapped_ds)}): \" + (\", \".join(unmapped_ds) if unmapped_ds else \"None\") + \"\\n\\n\")\n",
    "# # \n",
    "#     log.write(\"### Distribution Metadata Fields Mapping\\n\")\n",
    "#     log.write(f\"- Exact Matches ({len(exact_dist)}): \" + (\", \".join(exact_dist) if exact_dist else \"None\") + \"\\n\")\n",
    "#     log.write(f\"- Partial/Close Matches ({len(partial_dist)}): \" + (\", \".join(partial_dist) if partial_dist else \"None\") + \"\\n\")\n",
    "#     log.write(f\"- Problematic/Unmapped ({len(unmapped_dist)}): \" + (\", \".join(unmapped_dist) if unmapped_dist else \"None\") + \"\\n\\n\")\n",
    "\n",
    "# print(\"Buckets ready: exact_*, partial_*, unmapped_*; summary appended to evaluation.log\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9353eef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Exact file/sheet/column names as in your workbook\n",
    "MAPPING_XLSX = Path(\"/home/aakash/NIC/nic-metadata-cleaning/sample_catalog_metadata/NIC_metadata_curation_master.xlsx\")\n",
    "# if not MAPPING_XLSX.exists():\n",
    "#     raise FileNotFoundError(\"NIC_metadata_curation_master (1).xlsx not found in working directory.\")\n",
    "\n",
    "SHEET_CAT  = \"catalog_metadata_mapping_table\"\n",
    "SHEET_DS   = \"dataset_metadata_mapping_table\"\n",
    "SHEET_DIST = \"distribution_metadata_mapping_table\"\n",
    "\n",
    "# Read mapping tables (reuse existing variables if already loaded)\n",
    "df_cat_map  = pd.read_excel(MAPPING_XLSX, sheet_name=SHEET_CAT)\n",
    "df_ds_map   = pd.read_excel(MAPPING_XLSX, sheet_name=SHEET_DS)\n",
    "df_dist_map = pd.read_excel(MAPPING_XLSX, sheet_name=SHEET_DIST)\n",
    "\n",
    "# Keep only rows with a Mapping Type recorded\n",
    "for _df in (df_cat_map, df_ds_map, df_dist_map):\n",
    "    _df.dropna(subset=[\"Mapping Type\"], inplace=True)\n",
    "    _df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Fixed NIC column names per sheet\n",
    "CAT_NIC_COL   = \"Equivalent OGD Catalog Metadata\"\n",
    "DS_NIC_COL    = \"Equivalent OGD Dataset Metadata\"\n",
    "DIST_NIC_COL  = \"Equivalent OGD Dataset Metadata\"  # as given\n",
    "\n",
    "# Fixed Standard columns per sheet\n",
    "CAT_DCAT_COL     = \"DCAT v3 Mapping\"\n",
    "CAT_DUBLIN_COL   = \"Dublin Core Element\"\n",
    "\n",
    "DS_DCAT_COL      = \"DCAT v3 Mapping\"\n",
    "DS_DUBLIN_COL    = \"Dublin Core Element\"  # dataset sheet's Dublin Core column label in your file\n",
    "\n",
    "DIST_DCAT_COL    = \"DCAT v3 Mapping\"\n",
    "DIST_DUBLIN_COL  = \"Dubline core element\"  # spelling as in sheet\n",
    "\n",
    "def _is_missing(val) -> bool:\n",
    "    return (val is None) or (isinstance(val, float) and pd.isna(val)) or (isinstance(val, str) and val.strip() == \"\")\n",
    "\n",
    "def proposed_fields(df_map: pd.DataFrame, nic_col: str, std_col: str,\n",
    "                    level_label: str, standard_label: str):\n",
    "    \"\"\"\n",
    "    Return:\n",
    "      - proposed_list: sorted unique list of standard fields present in std_col\n",
    "        where NIC field is missing/None in nic_col.\n",
    "      - df_proposed: tidy DataFrame for export/logging.\n",
    "    \"\"\"\n",
    "    # Hard fail if columns aren't present (no guessing)\n",
    "    for c in (nic_col, std_col, \"Mapping Type\"):\n",
    "        if c not in df_map.columns:\n",
    "            raise KeyError(f\"Expected column '{c}' not found in {level_label} mapping sheet.\")\n",
    "\n",
    "    rows = []\n",
    "    for _, r in df_map.iterrows():\n",
    "        nic_val = r[nic_col]\n",
    "        std_val = r[std_col]\n",
    "\n",
    "        if _is_missing(nic_val) and not _is_missing(std_val):\n",
    "            rows.append({\n",
    "                \"Metadata Level\": level_label,\n",
    "                \"Standard\": standard_label,\n",
    "                \"Standard Field\": str(std_val).strip(),\n",
    "                \"OGD/NIC Equivalent\": \"(None)\",  \n",
    "                \"Mapping Type\": str(r[\"Mapping Type\"]).strip(),\n",
    "                \"Notes/Action\": r.get(\"Action for NIC\", \"\") if pd.notna(r.get(\"Action for NIC\", \"\")) else \"\"\n",
    "            })\n",
    "\n",
    "    if rows:\n",
    "        df_proposed = pd.DataFrame(rows)\n",
    "        proposed_list = sorted(df_proposed[\"Standard Field\"].unique())\n",
    "    else:\n",
    "        df_proposed = pd.DataFrame(columns=[\n",
    "            \"Metadata Level\",\"Standard\",\"Standard Field\",\"OGD/NIC Equivalent\",\"Mapping Type\",\"Notes/Action\"\n",
    "        ])\n",
    "        proposed_list = []\n",
    "\n",
    "    return proposed_list, df_proposed\n",
    "\n",
    "# Compute per-level proposed fields for DCAT v3\n",
    "cat_dcat_list,  cat_dcat_df  = proposed_fields(df_cat_map,  CAT_NIC_COL,  CAT_DCAT_COL,   \"Catalog\",      \"DCAT v3\")\n",
    "ds_dcat_list,   ds_dcat_df   = proposed_fields(df_ds_map,   DS_NIC_COL,   DS_DCAT_COL,    \"Dataset\",      \"DCAT v3\")\n",
    "dist_dcat_list, dist_dcat_df = proposed_fields(df_dist_map, DIST_NIC_COL, DIST_DCAT_COL,  \"Distribution\", \"DCAT v3\")\n",
    "\n",
    "# Compute per-level proposed fields for Dublin Core\n",
    "cat_dc_list,  cat_dc_df  = proposed_fields(df_cat_map,  CAT_NIC_COL,  CAT_DUBLIN_COL,   \"Catalog\",      \"Dublin Core\")\n",
    "ds_dc_list,   ds_dc_df   = proposed_fields(df_ds_map,   DS_NIC_COL,   DS_DUBLIN_COL,    \"Dataset\",      \"Dublin Core\")\n",
    "dist_dc_list, dist_dc_df = proposed_fields(df_dist_map, DIST_NIC_COL, DIST_DUBLIN_COL,  \"Distribution\", \"Dublin Core\")\n",
    "\n",
    "# Stack tidy outputs for each standard\n",
    "df_proposed_dcat   = pd.concat([cat_dcat_df, ds_dcat_df, dist_dcat_df], ignore_index=True)\n",
    "df_proposed_dublin = pd.concat([cat_dc_df,   ds_dc_df,   dist_dc_df],   ignore_index=True)\n",
    "\n",
    "\n",
    "log_path = Path(\"evaluation.log\")\n",
    "with log_path.open(\"a\", encoding=\"utf-8\") as log:\n",
    "    log.write(\"### Proposed New Fields (present in DCAT/Dublin, missing in OGD)\\n\\n\")\n",
    "\n",
    "    # DCAT v3\n",
    "    log.write(\"#### DCAT v3\\n\")\n",
    "    log.write(f\"- Catalog ({len(cat_dcat_list)}): \" + (\", \".join(cat_dcat_list) if cat_dcat_list else \"None\") + \"\\n\")\n",
    "    log.write(f\"- Dataset ({len(ds_dcat_list)}): \" + (\", \".join(ds_dcat_list) if ds_dcat_list else \"None\") + \"\\n\")\n",
    "    log.write(f\"- Distribution ({len(dist_dcat_list)}): \" + (\", \".join(dist_dcat_list) if dist_dcat_list else \"None\") + \"\\n\\n\")\n",
    "\n",
    "    # Dublin Core\n",
    "    log.write(\"#### Dublin Core\\n\")\n",
    "    log.write(f\"- Catalog ({len(cat_dc_list)}): \" + (\", \".join(cat_dc_list) if cat_dc_list else \"None\") + \"\\n\")\n",
    "    log.write(f\"- Dataset ({len(ds_dc_list)}): \" + (\", \".join(ds_dc_list) if ds_dc_list else \"None\") + \"\\n\")\n",
    "    log.write(f\"- Distribution ({len(dist_dc_list)}): \" + (\", \".join(dist_dc_list) if dist_dc_list else \"None\") + \"\\n\\n\")\n",
    "\n",
    "print(\"Prepared proposed-field outputs: df_proposed_dcat and df_proposed_dublin; summary appended to evaluation.log\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c53955",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reuse NIC column names (define if missing, without touching files)\n",
    "if \"CAT_NIC_COL\" not in locals():  CAT_NIC_COL  = \"Equivalent OGD Catalog Metadata\"\n",
    "if \"DS_NIC_COL\"  not in locals():  DS_NIC_COL   = \"Equivalent OGD Dataset Metadata\"\n",
    "if \"DIST_NIC_COL\" not in locals(): DIST_NIC_COL = \"Equivalent OGD Dataset Metadata\"\n",
    "\n",
    "\n",
    "def _is_missing(v):\n",
    "    if isinstance(v, str):\n",
    "        return v.strip() == \"\"\n",
    "    return pd.isna(v)\n",
    "def _proposed_list(df_prop: pd.DataFrame, level: str) -> list[str]:\n",
    "    if df_prop is None or df_prop.empty:\n",
    "        return []\n",
    "    return sorted(df_prop.loc[df_prop[\"Metadata Level\"] == level, \"Standard Field\"].dropna().astype(str).str.strip().unique())\n",
    "\n",
    "# If the explicit *_list variables exist (from earlier), keep them; else derive from DataFrames\n",
    "if \"cat_dcat_list\" not in locals():\n",
    "    cat_dcat_list  = _proposed_list(df_proposed_dcat,   \"Catalog\")\n",
    "    ds_dcat_list   = _proposed_list(df_proposed_dcat,   \"Dataset\")\n",
    "    dist_dcat_list = _proposed_list(df_proposed_dcat,   \"Distribution\")\n",
    "\n",
    "if \"cat_dc_list\" not in locals():\n",
    "    cat_dc_list  = _proposed_list(df_proposed_dublin, \"Catalog\")\n",
    "    ds_dc_list   = _proposed_list(df_proposed_dublin, \"Dataset\")\n",
    "    dist_dc_list = _proposed_list(df_proposed_dublin, \"Distribution\")\n",
    "\n",
    "\n",
    "def _counts_from_map(df_map: pd.DataFrame, nic_col: str) -> dict[str, int]:\n",
    "    m = df_map[\"Mapping Type\"].astype(str).str.lower()\n",
    "    exact    = m.str.contains(\"exact\", na=False)\n",
    "    partial  = m.str.contains(\"partial|closest\", na=False)\n",
    "    unmapped = m.str.contains(\"no match|no direct\", na=False)\n",
    "    return {\n",
    "        \"exact\":    int(exact.sum()),\n",
    "        \"partial\":  int(partial.sum()),\n",
    "        \"unmapped\": int(unmapped.sum()),\n",
    "        \"total\":    int((exact | partial | unmapped).sum())\n",
    "    }\n",
    "\n",
    "buckets = {\n",
    "    \"Catalog\":      _counts_from_map(df_cat_map,  CAT_NIC_COL),\n",
    "    \"Dataset\":      _counts_from_map(df_ds_map,   DS_NIC_COL),\n",
    "    \"Distribution\": _counts_from_map(df_dist_map, DIST_NIC_COL),\n",
    "}\n",
    "df_counts = pd.DataFrame(buckets).T[[\"exact\",\"partial\",\"unmapped\",\"total\"]].astype(int)\n",
    "\n",
    "# Coverage %\n",
    "df_pct = df_counts.copy()\n",
    "for col in [\"exact\",\"partial\",\"unmapped\"]:\n",
    "    df_pct[col] = (df_counts[col] / df_counts[\"total\"]).where(df_counts[\"total\"]>0, 0.0) * 100.0\n",
    "df_pct = df_pct[[\"exact\",\"partial\",\"unmapped\"]]\n",
    "\n",
    "# Transformation Effort Score (Exact=0, Partial=1, Unmapped=3)\n",
    "weights = {\"exact\": 0, \"partial\": 1, \"unmapped\": 3}\n",
    "df_tes = (\n",
    "    df_counts[[\"exact\",\"partial\",\"unmapped\"]]\n",
    "    .mul(pd.Series(weights))\n",
    "    .sum(axis=1)\n",
    "    .div(df_counts[\"total\"].replace({0:1}))\n",
    "    .rename(\"TES\")\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\":\"Level\"})\n",
    ")\n",
    "\n",
    "# Proposed fields counts per standard\n",
    "levels = [\"Catalog\",\"Dataset\",\"Distribution\"]\n",
    "df_proposed_counts = pd.DataFrame({\n",
    "    \"Level\": levels,\n",
    "    \"DCAT v3\": [len(cat_dcat_list), len(ds_dcat_list), len(dist_dcat_list)],\n",
    "    \"Dublin Core\": [len(cat_dc_list), len(ds_dc_list), len(dist_dc_list)],\n",
    "})\n",
    "\n",
    "# DCAT↔Dublin overlap & Jaccard\n",
    "def _jacc(a, b):\n",
    "    A, B = set(a), set(b)\n",
    "    inter = len(A & B)\n",
    "    union = len(A | B) or 1\n",
    "    return inter, union, inter/union\n",
    "\n",
    "over_cat  = _jacc(cat_dcat_list,  cat_dc_list)\n",
    "over_ds   = _jacc(ds_dcat_list,   ds_dc_list)\n",
    "over_dist = _jacc(dist_dcat_list, dist_dc_list)\n",
    "df_overlap = pd.DataFrame({\n",
    "    \"Level\": levels,\n",
    "    \"Intersection\": [over_cat[0], over_ds[0], over_dist[0]],\n",
    "    \"Union\":        [over_cat[1], over_ds[1], over_dist[1]],\n",
    "    \"Jaccard\":      [over_cat[2], over_ds[2], over_dist[2]],\n",
    "})\n",
    "\n",
    "# Duplication diagnostics (same NIC field mapped >1 time)\n",
    "def _dup_counts(df_map, nic_col):\n",
    "    s = pd.Series(df_map[nic_col])\n",
    "    s = s[~s.map(_is_missing)]\n",
    "    vc = s.value_counts()\n",
    "    repeated = vc[vc > 1]\n",
    "    return repeated.sum(), repeated.shape[0]\n",
    "\n",
    "dup_cat_total, dup_cat_unique   = _dup_counts(df_cat_map,  CAT_NIC_COL)\n",
    "dup_ds_total,  dup_ds_unique    = _dup_counts(df_ds_map,   DS_NIC_COL)\n",
    "dup_dist_total, dup_dist_unique = _dup_counts(df_dist_map, DIST_NIC_COL)\n",
    "\n",
    "df_dup = pd.DataFrame({\n",
    "    \"Level\": levels,\n",
    "    \"Repeated Mappings (rows)\": [dup_cat_total, dup_ds_total, dup_dist_total],\n",
    "    \"Fields Reused (unique)\":   [dup_cat_unique, dup_ds_unique, dup_dist_unique],\n",
    "})\n",
    "# 1) Mapping counts\n",
    "plt.figure()\n",
    "df_counts[[\"exact\",\"partial\",\"unmapped\"]].plot(kind=\"bar\")\n",
    "plt.title(\"Mapping Counts by Level (Exact / Partial / Unmapped)\")\n",
    "plt.xlabel(\"Metadata Level\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend(title=\"Category\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2) Coverage %\n",
    "plt.figure()\n",
    "df_pct.plot(kind=\"bar\")\n",
    "plt.title(\"Coverage % by Level (Exact / Partial / Unmapped)\")\n",
    "plt.xlabel(\"Metadata Level\")\n",
    "plt.ylabel(\"Percentage (%)\")\n",
    "plt.legend(title=\"Category\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3) Transformation Effort Score\n",
    "plt.figure()\n",
    "plt.bar(df_tes[\"Level\"], df_tes[\"TES\"])\n",
    "plt.title(\"Transformation Effort Score (TES) by Level\")\n",
    "plt.xlabel(\"Metadata Level\")\n",
    "plt.ylabel(\"TES (0=easy → higher=harder)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4) Proposed fields: DCAT v3 vs Dublin Core\n",
    "plt.figure()\n",
    "x = range(len(df_proposed_counts[\"Level\"]))\n",
    "w = 0.35\n",
    "plt.bar([i - w/2 for i in x], df_proposed_counts[\"DCAT v3\"], w, label=\"DCAT v3\")\n",
    "plt.bar([i + w/2 for i in x], df_proposed_counts[\"Dublin Core\"], w, label=\"Dublin Core\")\n",
    "plt.title(\"Proposed New Fields by Level\")\n",
    "plt.xlabel(\"Metadata Level\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(list(x), df_proposed_counts[\"Level\"])\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5) DCAT↔Dublin Jaccard\n",
    "plt.figure()\n",
    "plt.bar(df_overlap[\"Level\"], df_overlap[\"Jaccard\"])\n",
    "plt.title(\"Proposal Similarity (DCAT vs Dublin) by Level\")\n",
    "plt.xlabel(\"Metadata Level\")\n",
    "plt.ylabel(\"Jaccard Index (0–1)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 6) Duplication diagnostics\n",
    "plt.figure()\n",
    "plt.bar(df_dup[\"Level\"], df_dup[\"Fields Reused (unique)\"])\n",
    "plt.title(\"NIC Field Reuse (Unique Fields Appearing >1x)\")\n",
    "plt.xlabel(\"Metadata Level\")\n",
    "plt.ylabel(\"Count of Reused NIC Fields\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ----------------------- SAVE STATS & APPEND TO LOG -----------------------\n",
    "df_counts.to_csv(\"mapping_counts_by_level.csv\", index=True)\n",
    "df_pct.to_csv(\"mapping_coverage_percent.csv\", index=True)\n",
    "df_tes.to_csv(\"transformation_effort_score.csv\", index=False)\n",
    "df_proposed_counts.to_csv(\"proposed_fields_counts.csv\", index=False)\n",
    "df_overlap.to_csv(\"proposed_fields_overlap.csv\", index=False)\n",
    "df_dup.to_csv(\"nic_field_duplication_stats.csv\", index=False)\n",
    "\n",
    "with log_path.open(\"a\", encoding=\"utf-8\") as log:\n",
    "    log.write(\"### Transformation Stats & Visuals Summary\\n\\n\")\n",
    "    for lvl, row in df_counts.iterrows():\n",
    "        log.write(f\"- {lvl}: Exact={row['exact']}, Partial={row['partial']}, Unmapped={row['unmapped']}, Total={row['total']}\\n\")\n",
    "    log.write(\"\\n\")\n",
    "    for _, r in df_tes.iterrows():\n",
    "        log.write(f\"- TES {r['Level']}: {r['TES']:.2f}\\n\")\n",
    "    log.write(\"\\nSaved CSVs: mapping_counts_by_level.csv, mapping_coverage_percent.csv, transformation_effort_score.csv, \"\n",
    "              \"proposed_fields_counts.csv, proposed_fields_overlap.csv, nic_field_duplication_stats.csv\\n\\n\")\n",
    "\n",
    "print(\"Visuals rendered. Stats saved and summary appended to evaluation.log.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
